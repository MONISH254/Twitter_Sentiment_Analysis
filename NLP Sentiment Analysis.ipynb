{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1. Project Overview\n",
    "\n",
    "This project focuses on analyzing public sentiment from Tweets regarding Apple and Google products. The main goal is to build a Natural Language Processing (NLP) model that can classify the sentiment of a Tweet as positive, negative, or neutral based on its content.\n",
    "\n",
    "The data, sourced from [CrowdFlower](https://data.world/crowdflower/brands-and-product-emotions), contains over 9,000 Tweets labeled by human raters. By analyzing this data, the model will help Apple and Google better understand customer perceptions of their products, allowing them to make informed decisions for marketing, customer service, and product development.\n",
    "\n",
    "We will begin by preprocessing the Tweets, transforming them into a numerical format suitable for machine learning models, and training several classification algorithms to evaluate their performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Business Problem & Stakeholder\n",
    "\n",
    "### Business Problem:\n",
    "Public perception of tech products can heavily influence a company's sales, customer satisfaction, and brand loyalty. For companies like Apple and Google, understanding how customers feel about their products can provide valuable insights into areas for improvement, marketing strategies, and product development. \n",
    "\n",
    "In this project, we aim to develop a model that automatically classifies the sentiment of Tweets regarding Apple and Google products as positive, negative, or neutral. This can help companies quickly gauge public sentiment at scale, providing actionable insights for decision-making.\n",
    "\n",
    "### Stakeholders:\n",
    "- **Apple and Google Product Teams:** Use sentiment data to improve products and address customer pain points.\n",
    "- **Marketing Departments:** Tailor campaigns to target sentiment-driven messaging.\n",
    "- **Customer Support Teams:** Identify negative feedback more quickly to address concerns.\n",
    "- **Executives/Decision Makers:** Gain a high-level view of public opinion, enabling better strategic planning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Dataset Description\n",
    "\n",
    "The dataset used in this project comes from [CrowdFlower](https://data.world/crowdflower/brands-and-product-emotions), containing over 9,000 Tweets. Each Tweet has been labeled by human raters with one of three sentiment categories: positive, negative, or neither. The dataset contains the following key columns:\n",
    "\n",
    "### Key Features of the Dataset:\n",
    "- **Text**: The actual content of the Tweet, which we will analyze for sentiment classification.\n",
    "- **Brand/Product**: The specific product or brand mentioned in the Tweet, such as `iPhone`, `Google`, or `iPad`.\n",
    "- **Emotion**: The sentiment label, representing whether the sentiment expressed in the Tweet is `Positive emotion`, `Negative emotion`, or neutral.\n",
    "\n",
    "### Target Variable:\n",
    "- **Emotion**: This will be the target variable, as it captures the sentiment associated with each Tweet.\n",
    "\n",
    "The dataset will be used to train and evaluate models that can predict the sentiment of unseen Tweets based on their text content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Objectives\n",
    "\n",
    "### Specific Objective\n",
    "- Develop a Natural Language Processing (NLP) model to accurately classify the sentiment of Tweets about Apple and Google products into three categories: positive, negative, and neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Objectives\n",
    "\n",
    "1. **Data Preprocessing**: Clean and prepare the Tweet data for analysis, which includes handling missing values, normalizing text (removing URLs, special characters, etc.), and tokenization.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**: Conduct exploratory analysis to understand the distribution of sentiments in the dataset, identify any patterns, and visualize key aspects of the data.\n",
    "\n",
    "3. **Model Development**: Implement and train various classification models (e.g., Logistic Regression, Support Vector Machines, and Naive Bayes) to classify the sentiment of Tweets.\n",
    "\n",
    "4. **Model Evaluation**: Evaluate the performance of the models using appropriate metrics such as accuracy, precision, recall, and F1 score, especially focusing on multiclass classification metrics.\n",
    "\n",
    "5. **Iterative Improvement**: Based on initial results, refine the models by incorporating advanced techniques such as feature engineering, hyperparameter tuning, or using pre-trained embeddings like Word2Vec or BERT.\n",
    "\n",
    "6. **Conclusion and Recommendations**: Summarize the findings, discuss the model's effectiveness, and provide actionable recommendations for stakeholders based on the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Understanding & Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Data Loading & Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will:\n",
    "\n",
    "1. Load the dataset.\n",
    "2. Display the first few rows to understand the structure.\n",
    "3. Check the data types of each column.\n",
    "4. Identify missing values and duplicates.\n",
    "5. Analyze the class distribution to check for potential imbalance.\n",
    "6. Perform basic descriptive statistics (e.g., distribution of Tweet lengths and word counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')  # for nicer visualizations\n",
    "sns.set_palette(\"Set2\")  # Set Seaborn color palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "raw_data = pd.read_csv(\"judge-1377884607_tweet_product_company.csv\", encoding = \"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names seem to be unnecessarily long, we'll rename them to improve readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns\n",
    "raw_data.columns = [\"Text\", \"Brand/Product\", \"Emotion\"]\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of raws and columns\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting an overview of the data types\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates based on the Text column\n",
    "duplicates = raw_data.duplicated(subset='Text').sum()\n",
    "print(f\"Number of duplicate rows based on the text column: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in 'Text' column\n",
    "raw_data['Text'] = raw_data['Text'].fillna('')  # Replace NaN with an empty string\n",
    "\n",
    "# Descriptive statistics for tweet length and word count\n",
    "raw_data['tweet_length'] = raw_data['Text'].apply(len)\n",
    "raw_data['word_count'] = raw_data['Text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Tweet Length Statistics:\")\n",
    "print(raw_data['tweet_length'].describe())\n",
    "\n",
    "print(\"Word Count Statistics:\")\n",
    "print(raw_data['word_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Average tweet length** is *105 characters* with a max of *178 characters*.\n",
    "- **Average word count** is around *18 words*, with a maximum of *33 words*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will:\n",
    "\n",
    "- Handle missing values in the `Brand/Product` column.\n",
    "- Remove any duplicate rows.\n",
    "- Clean the text data by removing URLs, mentions, and special characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "Since the `Brand/Product` column contains a large number of missing values, we can consider one of the following approaches:\n",
    "\n",
    "- Dropping the column entirely if it's not necessary for the analysis.\n",
    "- Imputing values, but this might be challenging as this is categorical text data.\n",
    "\n",
    "For now, since this column is not critical to sentiment analysis, we'll drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'Brand/Product' column due to many missing values\n",
    "data_cleaned = raw_data.drop(columns=['Brand/Product'])\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicates\n",
    "\n",
    "We'll remove the duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate rows\n",
    "data_cleaned = data_cleaned.drop_duplicates()\n",
    "print(f\"Number of rows after removing duplicates: {data_cleaned.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Text Column\n",
    "\n",
    "To prepare the Text Column for modeling, we'll:\n",
    "\n",
    "- Remove URLs, mentions, and special characters.\n",
    "- Convert text to lowercase for uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean Text Column\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)  # Remove hashtags (optional)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters and numbers\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.strip()  # Remove leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# Applying the cleaning function to the 'Text' column\n",
    "data_cleaned['cleaned_text'] = data_cleaned['Text'].apply(clean_text)\n",
    "data_cleaned[['Text', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Emotion Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the occurence of unique values\n",
    "data_cleaned[\"Emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned[\"Emotion\"] = data_cleaned[\"Emotion\"].replace({\n",
    "    \"No emotion toward brand or product\": \"Neutral\",\n",
    "    \"Positive emotion\": \"Positive\",\n",
    "    \"Negative emotion\": \"Negative\"\n",
    "})\n",
    "\n",
    "data_cleaned[\"Emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting display options to show the full text\n",
    "pd.set_option('display.max_colwidth', None)  \n",
    "\n",
    "# Displaying the tweets where Emotion is \"I can't tell\"\n",
    "cant_tell_tweets = data_cleaned[data_cleaned[\"Emotion\"] == \"I can't tell\"][\"cleaned_text\"]\n",
    "print(cant_tell_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where the Emotion is \"I can't tell\"\n",
    "data_cleaned = data_cleaned[data_cleaned[\"Emotion\"] != \"I can't tell\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On investigating the text with the cant tell emotion we decided to drop the rows because there was no clear sentiment information, as some of the texts seemed sarcastic making them less useful for accurate analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Sentiment Class Distribution Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will visualize the sentiment class distribution to better understand the balance between different sentiment categories in the dataset. We will use a bar chart to display the count of each sentiment label: `positive`, `negative`, and `neutral`.\n",
    "\n",
    "The bar chart will help us assess the overall distribution and check for any significant class imbalance, which may affect our modeling process later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting plot size and style\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotting sentiment distribution using Seaborn's countplot\n",
    "sns.countplot(x='Emotion', \n",
    "              data= data_cleaned, \n",
    "              order= data_cleaned['Emotion'].value_counts().index)\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Sentiment Distribution', fontsize=16)\n",
    "plt.xlabel('Sentiment', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "\n",
    "# Rotating x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment distribution graph clearly shows that the dataset is imbalanced, with the majority of tweets labeled as `Neutral`, followed by `Positive emotion,` while `Negative emotion` labels has significantly fewer instances.\n",
    "\n",
    "This imbalance is something we'll need to address during the modeling phase, potentially through techniques like resampling or adjusting class weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Further Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now:\n",
    "\n",
    "- **Distribution of Tweet Lengths:** A histogram to show how the lengths of the tweets are distributed.\n",
    "- **Word Count Distribution:** A similar histogram for word count distribution.\n",
    "- **Correlation Between Tweet Length and Sentiment:** Boxplots to visualize how the length of the tweets varies with sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Distribution of Tweet Lengths\n",
    "This visualization will help us understand how the lengths of the tweets are distributed in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting distribution of tweet lengths\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data_cleaned['tweet_length'], bins=30, kde=True, color='purple')\n",
    "plt.title('Distribution of Tweet Lengths', fontsize=16)\n",
    "plt.xlabel('Tweet Length', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram above illustrates the distribution of tweet lengths within the dataset. Key observations include:\n",
    "\n",
    "- The distribution appears to be roughly normal, with a peak around 100-125 characters.\n",
    "- Most tweets fall within the range of 75 to 140 characters, suggesting that the average tweet is relatively concise, likely conforming to Twitter's character limits.\n",
    "- There are fewer tweets on both extremes (very short and very long), with a noticeable decline in frequency as the tweet length approaches the maximum of 178 characters.\n",
    "- The data shows some variability, but the majority of tweets are clustered around the mean, indicating a consistent tweeting style among users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Distribution of Word Count in Tweets\n",
    "We will now visualize the word count in tweets, which helps us understand how concise or detailed the tweets are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting distribution of word count\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data_cleaned['word_count'], bins=30, kde=True, color='blue')\n",
    "plt.title('Distribution of Word Count in Tweets', fontsize=16)\n",
    "plt.xlabel('Word Count', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows that most tweets contain between 10 and 25 words, with a peak around 20 words. Shorter tweets (around 10 words) are also common, while very short or long tweets are rare. The distribution approximates a normal curve centered at 20 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Correlation Between Tweet Length and Sentiment\n",
    "To understand if tweet length has any relation to sentiment, we will visualize the correlation using boxplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of tweet length by sentiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Emotion', y='tweet_length', data=data_cleaned)\n",
    "plt.title('Tweet Length vs. Sentiment', fontsize=16)\n",
    "plt.xlabel('Sentiment', fontsize=12)\n",
    "plt.ylabel('Tweet Length', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Tokenization and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this step is to prepare the text for NLP tasks by transforming it into a consistent and analyzable format. We achieve this by:\n",
    "\n",
    "1. Tokenization: Splitting the text into individual tokens (words).\n",
    "2. Normalization: Converting text to lowercase and removing punctuation, numbers, and special characters.\n",
    "\n",
    "After applying these steps, the text will be standardized, allowing for better processing in subsequent steps like vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')  # Ensure you have the required tokenizer\n",
    "\n",
    "# Tokenization and Normalization Function\n",
    "def tokenize_and_normalize(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation, numbers, and special characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply the function to the cleaned text column\n",
    "data_cleaned['tokens'] = data_cleaned['cleaned_text'].apply(tokenize_and_normalize)\n",
    "\n",
    "# Display the tokenized text\n",
    "data_cleaned[['cleaned_text', 'tokens']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will remove common words (stopwords) that don't carry much meaningful information for text analysis. Stopwords are often words like `\"the\", \"is\", \"in\",` etc., which are frequently used but donâ€™t contribute much to the overall meaning of a text.\n",
    "\n",
    "**Goal:** Improve the quality of our text data by eliminating such words, leaving only the key terms that carry the most important meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the set of English stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords from tokenized text\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Apply the function to the tokens column\n",
    "data_cleaned['tokens_no_stopwords'] = data_cleaned['tokens'].apply(remove_stopwords)\n",
    "\n",
    "# Display the first few rows to check the result\n",
    "data_cleaned[['tokens', 'tokens_no_stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will apply stemming or lemmatization to further normalize the tokens.\n",
    "\n",
    "Stemming reduces words to their root form by removing suffixes (e.g., \"running\" becomes \"run\"). It's a more aggressive approach but might produce non-existent words.\n",
    "Lemmatization is more linguistically accurate, converting words to their base or dictionary form (e.g., \"running\" becomes \"run\", and \"better\" becomes \"good\"). This method ensures the resulting words are real words.\n",
    "\n",
    "For this project, we will use lemmatization as it tends to preserve the meaning better than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary NLTK downloader\n",
    "import nltk\n",
    "\n",
    "# Download the 'averaged_perceptron_tagger' resource for POS tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')  # Download WordNet for lemmatization\n",
    "nltk.download('omw-1.4')  # Optional: Download for expanded WordNet support\n",
    "\n",
    "# Now, proceed with the original code\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Helper function to get part of speech (POS) tags for more accurate lemmatization\n",
    "def get_pos_tag(word):\n",
    "    # Mapping from POS tag to first character for lemmatizer\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Function to apply lemmatization to tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token, get_pos_tag(token)) for token in tokens]\n",
    "\n",
    "# Apply the lemmatization function to the tokens without stopwords\n",
    "data_cleaned['tokens_lemmatized'] = data_cleaned['tokens_no_stopwords'].apply(lemmatize_tokens)\n",
    "\n",
    "# Display the first few rows to check the result\n",
    "data_cleaned[['tokens_no_stopwords', 'tokens_lemmatized']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for Vectorization\n",
    "\n",
    "- **Ensure Cleaned Data is Present:** We will make sure that the cleaned data (after tokenization, normalization, stopwords removal, and lemmatization) is correctly defined and contains meaningful content.\n",
    "\n",
    "- **Perform Vectorization:** Use `CountVectorizer` for `Bag of Words` representation and `TfidfVectorizer` for `TF-IDF` representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming 'tokens_lemmatized' is a column in your data_cleaned DataFrame\n",
    "# and 'Emotion' is your target variable\n",
    "\n",
    "# Join tokens (which are lists) back into strings\n",
    "data_cleaned['tokens_lemmatized_str'] = data_cleaned['tokens_lemmatized'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "X = data_cleaned['tokens_lemmatized_str']  # Features (converted to strings)\n",
    "y = data_cleaned['Emotion']  # Target variable (emotion)\n",
    "\n",
    "# Train-test split (adjust test size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize TfidfVectorizer for TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data, and transform the test data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Convert to DataFrame for better readability if needed (optional)\n",
    "tfidf_train_df = pd.DataFrame(X_train_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "tfidf_test_df = pd.DataFrame(X_test_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Display TF-IDF representation for the training set\n",
    "print(\"\\nTF-IDF Representation (Training Set):\")\n",
    "print(tfidf_train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key points about the output:\n",
    "\n",
    "- **Sparsity:** Most of the values are 0.0, indicating that many words from the vocabulary don't appear in individual documents. This is common with text data, where only a small subset of words is present in any given document.\n",
    "\n",
    "- **Vocabulary size:** In this case, the training set has 6,925 unique words (columns). These are the words that were identified from the training data and assigned TF-IDF scores.\n",
    "\n",
    "- **Non-zero values:** When a word appears in a document, it is assigned a non-zero TF-IDF value. The higher the value, the more relevant that word is to the document in relation to the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Class Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Join tokens (which are lists) back into strings\n",
    "data_cleaned['tokens_lemmatized_str'] = data_cleaned['tokens_lemmatized'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "X = data_cleaned['tokens_lemmatized_str']  # Features (converted to strings)\n",
    "y = data_cleaned['Emotion']  # Target variable (emotion)\n",
    "\n",
    "# Train-test split (adjust test size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize TfidfVectorizer for TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data, and transform the test data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to balance the classes in the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Check the class distribution after applying SMOTE\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(y_train_balanced.value_counts())\n",
    "\n",
    "# After this we can now proceed with training a model using X_train_balanced and y_train_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes to confirm sizes before SMOTE\n",
    "print(\"Before SMOTE:\")\n",
    "print(f\"X_train_tfidf shape: {X_train_tfidf.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes after SMOTE to confirm sizes\n",
    "print(\"After SMOTE:\")\n",
    "print(f\"X_train_balanced shape: {X_train_balanced.shape}\")\n",
    "print(f\"y_train_balanced shape: {y_train_balanced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that the classes have been successfully balanced using SMOTE, with each class (Neutral, Positive, Negative) now having 4,328 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Model 1: Binary Classification (Positive vs. Negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Model Selection\n",
    "\n",
    "We will begin with a binary classifier focusing on distinguishing between Positive and Negative emotions. For this, let's select two baseline models:\n",
    "\n",
    "1. Naive Bayes (specifically `MultinomialNB` since we are dealing with text data).\n",
    "\n",
    "2. Logistic Regression.\n",
    "\n",
    "Let's start with these two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out Neutral samples for binary classification in the training set\n",
    "binary_train_mask = y_train_balanced != 'Neutral'\n",
    "binary_test_mask = y_test != 'Neutral'\n",
    "\n",
    "# Subset the training and test sets\n",
    "X_train_binary = X_train_balanced[binary_train_mask]\n",
    "y_train_binary = y_train_balanced[binary_train_mask]\n",
    "X_test_binary = X_test_tfidf[binary_test_mask]\n",
    "y_test_binary = y_test[binary_test_mask]\n",
    "\n",
    "# Convert target labels to binary (1 for Positive, 0 for Negative)\n",
    "y_train_binary = y_train_binary.map({'Positive': 1, 'Negative': 0})\n",
    "y_test_binary = y_test_binary.map({'Positive': 1, 'Negative': 0})\n",
    "\n",
    "# Initialize models\n",
    "nb_model = MultinomialNB()\n",
    "logreg_model = LogisticRegression(max_iter=1000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Model Training\n",
    "\n",
    "We will now train both models on the binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes\n",
    "nb_model.fit(X_train_binary, y_train_binary)\n",
    "\n",
    "# Train Logistic Regression\n",
    "logreg_model.fit(X_train_binary, y_train_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the performance of both models using several metrics:\n",
    "\n",
    "1. Accuracy\n",
    "2. Precision, Recall, and F1-Score for each class.\n",
    "3. ROC-AUC for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred_nb = nb_model.predict(X_test_binary)\n",
    "y_pred_logreg = logreg_model.predict(X_test_binary)\n",
    "\n",
    "# Evaluate Naive Bayes\n",
    "print(\"Naive Bayes Model Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_binary, y_pred_nb))\n",
    "print(\"\\nClassification Report (Naive Bayes):\")\n",
    "print(classification_report(y_test_binary, y_pred_nb))\n",
    "print(\"\\nConfusion Matrix (Naive Bayes):\")\n",
    "print(confusion_matrix(y_test_binary, y_pred_nb))\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "print(\"\\nLogistic Regression Model Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_binary, y_pred_logreg))\n",
    "print(\"\\nClassification Report (Logistic Regression):\")\n",
    "print(classification_report(y_test_binary, y_pred_logreg))\n",
    "print(\"\\nConfusion Matrix (Logistic Regression):\")\n",
    "print(confusion_matrix(y_test_binary, y_pred_logreg))\n",
    "\n",
    "# ROC-AUC Score for Logistic Regression\n",
    "y_pred_logreg_proba = logreg_model.predict_proba(X_test_binary)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test_binary, y_pred_logreg_proba)\n",
    "print(f\"ROC-AUC (Logistic Regression): {roc_auc}\")\n",
    "\n",
    "# Plot ROC Curve for Logistic Regression\n",
    "fpr, tpr, _ = roc_curve(y_test_binary, y_pred_logreg_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', label=f\"ROC Curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Logistic Regression')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison Summary\n",
    "\n",
    "1. **Naive Bayes:**\n",
    "\n",
    "- Accuracy: `82.7%`\n",
    "- Strength in predicting `Class 1 (Positive)` with a precision of `0.92`, but struggles with `Class 0 (Negative)` with a precision of `0.52`.\n",
    "- F1-score for Class 0 is relatively low `(0.58)`, reflecting issues with false positives.\n",
    "\n",
    "2. **Logistic Regression:**\n",
    "\n",
    "- Accuracy: `85.4%` (higher than Naive Bayes).\n",
    "- Better performance overall, especially for Class 0, with improved precision (`0.58`) and F1-score (`0.63`).\n",
    "- ROC-AUC: `0.87`, showing a good ability to distinguish between classes.\n",
    "\n",
    "**Conclusion:** Logistic Regression outperforms Naive Bayes, providing better precision, recall, and overall class balance, particularly in handling the minority class (Class 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the metrics for both models (precision, recall, f1-score)\n",
    "metrics_nb = {\n",
    "    'Class 0': {'precision': 0.52, 'recall': 0.67, 'f1-score': 0.58},\n",
    "    'Class 1': {'precision': 0.92, 'recall': 0.86, 'f1-score': 0.89}\n",
    "}\n",
    "\n",
    "metrics_lr = {\n",
    "    'Class 0': {'precision': 0.58, 'recall': 0.68, 'f1-score': 0.63},\n",
    "    'Class 1': {'precision': 0.93, 'recall': 0.89, 'f1-score': 0.91}\n",
    "}\n",
    "\n",
    "# Plot the comparison for precision, recall, and f1-score\n",
    "classes = ['Class 0', 'Class 1']\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    nb_scores = [metrics_nb[cls][metric] for cls in classes]\n",
    "    lr_scores = [metrics_lr[cls][metric] for cls in classes]\n",
    "\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(len(classes))\n",
    "\n",
    "    ax[i].bar(index, nb_scores, bar_width, label='Naive Bayes', color='blue')\n",
    "    ax[i].bar(index + bar_width, lr_scores, bar_width, label='Logistic Regression', color='orange')\n",
    "\n",
    "    ax[i].set_title(f'Comparison of {metric.capitalize()}')\n",
    "    ax[i].set_xlabel('Classes')\n",
    "    ax[i].set_ylabel(metric.capitalize())\n",
    "    ax[i].set_xticks(index + bar_width / 2)\n",
    "    ax[i].set_xticklabels(classes)\n",
    "    ax[i].legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Model 2: Multiclass Classification (Positive, Negative, Neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will perform multiclass classification using three models:\n",
    "\n",
    "- Multinomial Naive Bayes\n",
    "- Logistic Regression\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Use SMOTE balanced data (X_train_tfidf and y_train_smote)\n",
    "# Multinomial Naive Bayes Classifier\n",
    "nb_multiclass = MultinomialNB()\n",
    "nb_multiclass.fit(X_train_tfidf, y_train_balanced)\n",
    "\n",
    "# Logistic Regression Classifier\n",
    "log_reg_multiclass = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "log_reg_multiclass.fit(X_train_tfidf, y_train_balanced)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_multiclass = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_multiclass.fit(X_train_tfidf, y_train_balanced)\n",
    "\n",
    "# Test the models on the test set\n",
    "nb_pred = nb_multiclass.predict(X_test_tfidf)\n",
    "log_reg_pred = log_reg_multiclass.predict(X_test_tfidf)\n",
    "rf_pred = rf_multiclass.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation for Multinomial Naive Bayes\n",
    "print(\"Multinomial Naive Bayes Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, nb_pred))\n",
    "print(classification_report(y_test, nb_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, nb_pred))\n",
    "\n",
    "# Evaluation for Logistic Regression\n",
    "print(\"\\nLogistic Regression Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, log_reg_pred))\n",
    "print(classification_report(y_test, log_reg_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, log_reg_pred))\n",
    "\n",
    "# Evaluation for Random Forest\n",
    "print(\"\\nRandom Forest Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_pred))\n",
    "print(classification_report(y_test, rf_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1. Fit the models on the balanced training set\n",
    "nb_multiclass = MultinomialNB()\n",
    "nb_multiclass.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "log_reg_multiclass = LogisticRegression(max_iter=1000)\n",
    "log_reg_multiclass.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "rf_multiclass = RandomForestClassifier()\n",
    "rf_multiclass.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# 2. Make predictions using the fitted models\n",
    "nb_pred = nb_multiclass.predict(X_test_tfidf)\n",
    "log_reg_pred = log_reg_multiclass.predict(X_test_tfidf)\n",
    "rf_pred = rf_multiclass.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation for Multinomial Naive Bayes\n",
    "print(\"\\nMultinomial Naive Bayes Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, nb_pred))\n",
    "print(classification_report(y_test, nb_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, nb_pred))\n",
    "\n",
    "# Evaluation for Logistic Regression\n",
    "print(\"\\nLogistic Regression Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, log_reg_pred))\n",
    "print(classification_report(y_test, log_reg_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, log_reg_pred))\n",
    "\n",
    "# Evaluation for Random Forest\n",
    "print(\"\\nRandom Forest Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_pred))\n",
    "print(classification_report(y_test, rf_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, rf_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Observations\n",
    "\n",
    "- **Accuracy Comparison:**\n",
    "\n",
    "    Binary models (especially Logistic Regression) outperformed multiclass models, achieving higher accuracy rates `(up to 85.44% for Logistic Regression)` compared to the highest multiclass accuracy of `67.47% (Random Forest)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Class Prediction Strength:**\n",
    "\n",
    "    In binary classification, both models excelled at classifying Positive instances, while the multiclass models struggled with Negative class predictions across the board. Multinomial Naive Bayes, in particular, had low precision and F1-scores for the Negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the performance discrepancies, the next step will involve hyperparameter tuning to optimize the multiclass models, particularly focusing on improving the classification of the Negative class and overall model accuracy. This will be accomplished using `GridSearchCV` for hyperparameter optimization on the selected multiclass models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `GridSearchCV` to optimize hyperparameters for the multiclass models, focusing on the following models: *Multinomial Naive Bayes*, *Logistic Regression*, and *Random Forest*.\n",
    "\n",
    "We'll define a parameter grid for each model and use GridSearchCV to find the best parameters based on `cross-validated performance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grid_nb = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0]  # Smoothing parameter for MultinomialNB\n",
    "}\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
    "    'max_iter': [100, 200, 300]  # Maximum number of iterations for convergence\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "nb_model = MultinomialNB()\n",
    "lr_model = LogisticRegression()\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Initialize GridSearchCV for each model\n",
    "grid_nb = GridSearchCV(nb_model, param_grid_nb, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_lr = GridSearchCV(lr_model, param_grid_lr, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Fit the models on the balanced training data\n",
    "print(\"Tuning Multinomial Naive Bayes...\")\n",
    "grid_nb.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Tuning Logistic Regression...\")\n",
    "grid_lr.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Tuning Random Forest...\")\n",
    "grid_rf.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Best parameters for each model\n",
    "print(\"\\nBest parameters for Multinomial Naive Bayes:\", grid_nb.best_params_)\n",
    "print(\"Best parameters for Logistic Regression:\", grid_lr.best_params_)\n",
    "print(\"Best parameters for Random Forest:\", grid_rf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Advanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Experiment with Word2Vec or GloVe embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Use BERT for Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3. Model Evaluation: (comparison with the simpler models above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. SHAP or LIME Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SHAP or LIME to explain which features (words or n-grams) contributed most to model decisions.\n",
    "\n",
    "- *Visualize and interpret feature importance for each sentiment class.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Validation Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Final Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Business Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Model Performance Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
